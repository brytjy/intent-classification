{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = pd.read_csv('intent_data.csv', encoding = 'unicode_escape')\n",
    "raw = intents\n",
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents.groupby('intent').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents[\"text\"] = intents[\"text\"].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(tokens):\n",
    "    tokens_new = []\n",
    "    for token in tokens:\n",
    "        t = token.lower()\n",
    "        tokens_new.append(t)\n",
    "    return tokens_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents[\"text\"] = intents[\"text\"].apply(lambda x: lowercase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in text: \n",
    "        new_text = re.sub('[^A-Za-z0-9]+', '', x) # remove punc.         \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents[\"text\"] = intents[\"text\"].apply(lambda x: preprocess(x))\n",
    "intents[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_token = []\n",
    "    for t in text:\n",
    "        if t not in stop_words:\n",
    "            new_token.append(t)\n",
    "    return new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents[\"text\"] = intents[\"text\"].apply(lambda x: remove_stopwords(x))\n",
    "intents[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text):\n",
    "    new_token = []\n",
    "    for t in text:\n",
    "        word = lemmatizer.lemmatize(t)\n",
    "        new_token.append(word)\n",
    "    return new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents[\"text\"] = intents[\"text\"].apply(lambda x: lemma(x))\n",
    "intents[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = intents[\"intent\"]\n",
    "le = LabelEncoder()\n",
    "le.fit(Y)\n",
    "print(le.classes_)\n",
    "Y = le.transform(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(text):\n",
    "    string = \", \".join(text)\n",
    "    string = string.replace(\",\", \"\")\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents[\"text\"] = intents[\"text\"].apply(lambda x: join(x))\n",
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = intents[\"text\"]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(intents['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Tfidf = Tfidf_vect.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(X_Tfidf,Y,test_size=0.2)\n",
    "print(\"   X         Y\")\n",
    "print(Train_X.shape, Train_Y.shape) # 320 training samples\n",
    "print(Test_X.shape, Test_Y.shape) # 80 test samples\n",
    "print(type(Train_X), type(Train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X, Train_Y)\n",
    "predictions_NB = Naive.predict(Test_X)\n",
    "print(accuracy_score(predictions_NB, Test_Y)*100)\n",
    "print(classification_report(Test_Y, predictions_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,1,2,3,4,5]\n",
    "cm = confusion_matrix(Test_Y, predictions_NB, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X, Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X)\n",
    "print(accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(classification_report(Test_Y, predictions_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(Train_X.toarray(), Train_Y)\n",
    "predictions_LDA = LDA.predict(Test_X)\n",
    "print(accuracy_score(predictions_LDA, Test_Y)*100)\n",
    "print(classification_report(Test_Y, predictions_LDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN.fit(Train_X, Train_Y)\n",
    "predictions_KNN = KNN.predict(Test_X)\n",
    "print(accuracy_score(predictions_KNN, Test_Y)*100)\n",
    "print(classification_report(Test_Y, predictions_KNN))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
